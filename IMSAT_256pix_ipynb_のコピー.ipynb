{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMSAT_256pix.ipynb のコピー",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazuki-de/git_reserch_code/blob/master/IMSAT_256pix_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZF0xNLHyAw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation , Softmax, MaxPool2D,UpSampling2D, Input, BatchNormalization, add, AveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "import datetime\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3EGpbKQyI-u",
        "colab_type": "code",
        "outputId": "7c547127-61a5-4787-f263-ce49095e1d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNRCFB4gyP-G",
        "colab_type": "code",
        "outputId": "e5431926-e75b-4041-8121-9538d6ab2ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cd  /content/drive/My Drive/logs"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/logs'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X89NCoRJyQtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"/content/drive/My Drive/logs/gradient_tape\", histogram_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNk17qRXbK5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mnistなどのデータ読み込み\n",
        "mnist = tf.keras.datasets.mnist\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMF9Q1p4F0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rescell(data, filters, kernel_size, option=False):\n",
        "    strides=(1,1)\n",
        "    if option:\n",
        "        strides=(2,2)\n",
        "    x=Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(data)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=Activation('relu')(x)\n",
        "    \n",
        "    data=Conv2D(filters=int(x.shape[3]), kernel_size=(1,1), strides=strides, padding=\"same\")(data)\n",
        "    x=Conv2D(filters=filters,kernel_size=kernel_size,strides=(1,1),padding=\"same\")(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=add([x,data])\n",
        "    x=Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def ResNet(img_rows, img_cols, img_channels):\n",
        "\tinput=Input(shape=(img_rows,img_cols,img_channels))\n",
        "\tx=Conv2D(32,(7,7), padding=\"same\",activation=\"relu\")(input)\n",
        "\tx=MaxPool2D(pool_size=(2,2))(x)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\tx=rescell(x,64,(3,3))\n",
        "\tx=rescell(x,64,(3,3))\n",
        "\tx=rescell(x,64,(3,3))\n",
        "\n",
        "\tx=rescell(x,128,(3,3),True)\n",
        "\n",
        "\tx=rescell(x,128,(3,3))\n",
        "\tx=rescell(x,128,(3,3))\n",
        "\tx=rescell(x,128,(3,3))\n",
        "\n",
        "\tx=rescell(x,256,(3,3),True)\n",
        "\n",
        "\tx=rescell(x,256,(3,3))\n",
        "\tx=rescell(x,256,(3,3))\n",
        "\tx=rescell(x,256,(3,3))\n",
        "\tx=rescell(x,256,(3,3))\n",
        "\tx=rescell(x,256,(3,3))\n",
        "\n",
        "\tx=rescell(x,512,(3,3),True)\n",
        "\n",
        "\tx=rescell(x,512,(3,3))\n",
        "\tx=rescell(x,512,(3,3))\n",
        "\n",
        "\tx=AveragePooling2D(pool_size=(int(x.shape[1]),int(x.shape[2])),strides=(2,2))(x)\n",
        "\n",
        "\tx=Flatten()(x)\n",
        "\tx=Dense(units=10,kernel_initializer=\"he_normal\",activation=\"softmax\")(x)\n",
        "\tmodel=Model(inputs=input,outputs=[x])\n",
        "\treturn model\n",
        "model1 = ResNet(256,256,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdAPsKsftqUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = Conv2D(16,(7,7),padding=\"same\", activation='relu')\n",
        "        self.maxpool = MaxPool2D((2, 2), padding='same')\n",
        "        self.conv2 = Conv2D(16,(3,3),padding=\"same\", activation='relu')\n",
        "        self.maxpool2 = MaxPool2D((2, 2), padding='same')\n",
        "        self.conv3 = Conv2D(16,(3,3),padding=\"same\", activation='relu')\n",
        "        self.maxpool3 = MaxPool2D((2, 2), padding='same')\n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(128, activation='relu')\n",
        "        self.d2 = Dense(10)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = rescell(x,64,(3,3))\n",
        "        x = rescell(x,64,(3,3))\n",
        "        x = rescell(x,64,(3,3))\n",
        "\n",
        "        x = rescell(x,128,(3,3),True)\n",
        "\n",
        "        x= rescell(x,128,(3,3))\n",
        "        x= rescell(x,128,(3,3))\n",
        "        x= rescell(x,128,(3,3))\n",
        "\n",
        "        x = rescell(x,256,(3,3),True)\n",
        "\n",
        "        x = rescell(x,256,(3,3))\n",
        "        x=rescell(x,256,(3,3))\n",
        "        x=rescell(x,256,(3,3))\n",
        "        x=rescell(x,256,(3,3))\n",
        "        x=rescell(x,256,(3,3))\n",
        "\n",
        "        x=rescell(x,512,(3,3),True)\n",
        "\n",
        "        x=rescell(x,512,(3,3))\n",
        "        x=rescell(x,512,(3,3))\n",
        "\n",
        "        x=AveragePooling2D(pool_size=(int(x.shape[1]),int(x.shape[2])),strides=(2,2))(x)\n",
        "\n",
        "        x=self.flatten(x)\n",
        "        return self.d1(x)\n",
        "\n",
        "\n",
        "model2 = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6_5QyC_CzWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bcff05e0-0e90-4a76-a88c-85401ff78262"
      },
      "source": [
        "model2.layers"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd45e39f1d0>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd45e42ea58>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd45e42ec50>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd45e3d20f0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd45e3d22e8>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd45e3d2748>,\n",
              " <tensorflow.python.keras.layers.core.Flatten at 0x7fd45e3d2940>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fd45e3d2a58>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fd45e3d2d30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdDBcUNq0yCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kl_divergence(q_logi, p_logi):\n",
        "    q_logit = Activation(\"softmax\")(q_logi)\n",
        "    p_logit = Activation(\"softmax\")(p_logi)\n",
        "    qlogq = tf.reduce_sum(q_logit * tf.math.log(q_logit), axis=1)\n",
        "    qlogp = tf.reduce_sum(q_logit * tf.math.log(p_logit), axis=1)\n",
        "    return qlogq - qlogp\n",
        "def get_unit_vector(v):\n",
        "    return  v / (tf.sqrt(tf.reduce_sum(v ** 2, axis=None,keepdims=True) + 1e-16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fdnl19iDr7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_entropy(p):\n",
        "    return - tf.reduce_sum(p*tf.math.log(p+1e-16),axis = 1)\n",
        "\n",
        "def compute_marginal_entropy(p_batch):\n",
        "    return  compute_entropy(tf.reduce_mean(p_batch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qne_U9p5auUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#損失関数\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "#最適化手法\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "#評価関数\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_loss1 = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_loss2 = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2b7AlZVoXDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ミニバッチ学習==1epocでn枚の画像からx枚ずつ取り出しn/x回学習する(重みを更新していく)手法\n",
        "#バッチ学習 == 1epocでn枚の画像から1枚ずつ取り出しn回学習する（重みを更新していく）手法\n",
        "#オンライン学習 ==1epocでn枚の画像からn枚全てを用い学習する(重みを更新していく)手法　#メモリ消費大\n",
        "@tf.function\n",
        "def train_step(image,label,batch_size):\n",
        "    with tf.GradientTape() as tape:\n",
        "        p = Activation('softmax')(model1(image))\n",
        "        p_ave = tf.reduce_mean(p,axis=0)\n",
        "        loss_eq2 = -tf.reduce_sum(p_ave * tf.math.log(p_ave + 1e-16))\n",
        "        loss_eq1 = tf.reduce_mean(compute_entropy(p))\n",
        "        loss_eq = loss_eq1 - 4* loss_eq2\n",
        "        #with tf.variable_scope(tf.get_variable_scope(),reuse=True):\n",
        "        #logit = model1(image)\n",
        "        \n",
        "        #---------------------------------------------------------------------------------\n",
        "        plain_softmax = model1(image)\n",
        "        \n",
        "        noplain_softmax = Activation(\"softmax\")(plain_softmax)\n",
        "\n",
        "        #適当な単位ベクトルにxi=10を掛けた数:pertubationの作成\n",
        "        perturbation = 10 * get_unit_vector(tf.random.normal(shape=tf.shape(image), dtype=tf.float64))\n",
        "        #print(perturbation.numpy())\n",
        "        for i in range(1):\n",
        "            softmax_accommodating_perturbation = model1(image+perturbation)\n",
        "            softmax_accommodating_perturbation_af = Activation(\"softmax\")(softmax_accommodating_perturbation)\n",
        "            # ノイズを足した配列とノーマルな配列がCNNで出力された結果のKL距離を求める\n",
        "            dist = kl_divergence(noplain_softmax,softmax_accommodating_perturbation_af)\n",
        "            #cross_entropy_accommodating_perturbation = -tf.reduce_sum(plain_softmax * tf.math.log(clipped(softmax_accommodating_perturbation))) * weight\n",
        "            adversarial_direction = tf.gradients(dist, [perturbation])[0]\n",
        "            print(adversarial_direction)\n",
        "            pertubation = tf.stop_gradient(adversarial_direction)\n",
        "        #print(\"ok\")    \n",
        "\n",
        "        pertubation = 1 * get_unit_vector(adversarial_direction)\n",
        "        corrent_softmax = Activation('softmax')(model1(image)) \n",
        "        vat_softmax = Activation('softmax')(model1(image + perturbation))\n",
        "        VAT_loss = kl_divergence(corrent_softmax,vat_softmax)\n",
        "        #---------------------------------------------------------------------------------------------\n",
        "        \n",
        "        total_loss = VAT_loss +0.2 * loss_eq\n",
        "        #hy_ = compute_entropy(p)\n",
        "        #p_ave = tf.reduce_mean(p,axis=0)\n",
        "        #hy_x = -tf.reduce_sum(p_ave*tf.math.log(p_ave+1e-16))\n",
        "        #print(hy)\n",
        "        #hy_x = tf.reduce_sum(compute_entropy(p))/batch_size\n",
        "        #print(hy_x)\n",
        "        #Rsut = -tf.reduce_sum(VAT_KL(image,model1))/batch_size\n",
        "        #print(Rsut)\n",
        "        #loss = Rsut - 0.2*((4)*hy-hy_x)\n",
        "    gradients = tape.gradient(total_loss, model1.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model1.trainable_variables))\n",
        "    \n",
        "    train_loss(total_loss)\n",
        "    #train_loss1(Rsut)\n",
        "    #train_loss2(0.2*((4.0)*hy-hy_x))\n",
        "    #train_loss(loss)\n",
        "    #train_loss\n",
        "    \n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uXj-nnVgv8S",
        "colab_type": "code",
        "outputId": "11e00ed6-b6d4-4ba5-8d54-bc29430e7c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pip install munkres\n",
        "from munkres import Munkres, print_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: munkres in /usr/local/lib/python3.6/dist-packages (1.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLc0Gss0ZwKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(y_pred, y_t,tot_cl=10):\n",
        "    # compute the accuracy using Hungarian algorithm\n",
        "    m = Munkres()\n",
        "    mat = np.zeros((tot_cl, tot_cl))\n",
        "    for i in range(tot_cl):\n",
        "        for j in range(tot_cl):\n",
        "            mat[i][j] = np.sum(np.logical_and(y_pred == i, y_t == j))\n",
        "    indexes = m.compute(-mat)\n",
        "\n",
        "    corresp = []\n",
        "    for i in range(tot_cl):\n",
        "        corresp.append(indexes[i][1])\n",
        "\n",
        "    pred_corresp = [corresp[predicte] for predicted in y_pred]\n",
        "    acc = np.sum(pred_corresp == y_t) / float(len(y_t))\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHk9axm2bJJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoc =500\n",
        "\n",
        "for i in range(epoc):\n",
        "    for image,label in train_ds:\n",
        "        train_step(image,label,100)\n",
        "    #print( train_loss1.result())  \n",
        "    #print( train_loss2.result())\n",
        "    print( train_loss.result())\n",
        "    #print(compute_accuracy(np.argmax(Activation('softmax')(model1(x_train)).astype(np.float32),y_train.astype(np.float32).,10))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6vmiOxO0v0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/2048pix_tissue_choice/sumpling_20.csv\"\n",
        "def generater_train():\n",
        "    file_path1 = pd.read_csv(\"/content/drive/My Drive/2048pix_tissue_choice/tran_6000.csv\")\n",
        "    file_path1 = file_path1[0:300]\n",
        "    \n",
        "    # LabelEncode(classをint型に変換)するためのdict\\n\",\n",
        "    classes =[\"PI\",\"PP\",\"TRU\"]\n",
        "    classes = {v: i for i, v in enumerate(sorted(classes))}\n",
        "    #file_path1 = pd.read_csv(file_path)\n",
        "    length = len(file_path1)\n",
        "    for i in range(length):\n",
        "        path = file_path1.loc[i][\"path1\"]\n",
        "        with Image.open(path) as f:\n",
        "                tmp_image = np.asarray(f, dtype=np.float32)/255\n",
        "        label = file_path1.loc[i][\"type\"]\n",
        "        tmp_label = tf.keras.utils.to_categorical(classes[label],len(classes))\n",
        "        #print(tmp_labels, np.mean(tmp_images, axis=(1,2,3)))\n",
        "        yield tmp_image, tmp_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-oGkY5w1JPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "344a1f5f-a799-4958-a142-a7d16fcb24cc"
      },
      "source": [
        "#mnist用\n",
        "EPOCHS = 100\n",
        "#model = MyModel()\n",
        "batchsize = 10\n",
        "dataset_t = tf.data.Dataset.from_generator(generater_train, output_types=(tf.float32, tf.int32)).batch(1)\n",
        "#dataset_v = tf.data.Dataset.from_generator(generater_val, output_types=(tf.float32, tf.int32)).batch(10)\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch, (x_t,x_l) in enumerate(dataset_t):\n",
        "        train_step(x_t,x_l,1)\n",
        "        if batch%100 ==0:\n",
        "            print(batch)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ac97226bdd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    357\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    358\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 359\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   1542\u001b[0m         self._function_attributes)\n\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    714\u001b[0m                                           converted_func)\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    704\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in converted code:\n\n    <ipython-input-10-386c872916e4>:4 train_step  *\n        p = Activation('softmax')(model1(image))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-21-592411ed7204>:17 call  *\n        x =self.d1(x)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:616 __call__\n        self._maybe_build(inputs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1966 _maybe_build\n        self.build(input_shapes)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:1017 build\n        trainable=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:389 add_weight\n        aggregation=aggregation)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:713 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:154 make_variable\n        shape=variable_shape if variable_shape else None)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:260 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\n        shape=shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:60 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:139 __init__\n        initial_value() if init_from_fn else initial_value,\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:134 <lambda>\n        init_val = lambda: initializer(shape, dtype=dtype)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py:437 __call__\n        return self._random_generator.random_uniform(shape, -limit, limit, dtype)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py:800 random_uniform\n        shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py:247 random_uniform\n        rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py:809 random_uniform\n        _six.raise_from(_core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[67108864,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycTXOjlU6g_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.save_weights('/content/drive/My Drive/research_/weight_autoencoder_2048_0827/IMSAT_cifer10.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lONj1irYkVrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.load_weights('/content/drive/My Drive/research_/weight_autoencoder_2048_0827/IMSAT_mnist.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmenScDRzq_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VAT_KL(input_tensor, be_softmax, xi=10, epsilon=1.0, weight=1.0, num_approximation=1, clip_value_min=1e-16, dtype=tf.float64):\n",
        "    \n",
        "    #配列内の数値の最大値が第二変数になるように置き換える\n",
        "    #clipped = lambda x: tf.maximum(x, clip_value_min)\n",
        "    #axis_without_batch_size は (1,2,3)となる\n",
        "    #axis_without_batch_size = tuple(range(1,len(input_tensor.get_shape())))\n",
        "    \n",
        "    #if len(axis_without_batch_size) == 1:\n",
        "    #    axis_without_batch_size = axis_without_batch_size[0]\n",
        "        \n",
        "    #normalized = lambda x: x / clipped(tf.norm(x, axis=None))\n",
        "    #normalized = lambda x: x / clipped(tf.norm(x, axis=None,keepdims=True))\n",
        "    #normalized = lambda x: x / tf.norm(x, axis=None,keepdims=True)\n",
        "    #plain_softmax = network(input_tensor)\n",
        "    \n",
        "    #noplain_softmax = Activation(\"softmax\")(plain_softmax)\n",
        "    \n",
        "    #適当な単位ベクトルにxi=10を掛けた数:pertubationの作成\n",
        "    perturbation = xi * get_unit_vector(tf.random.normal(shape=tf.shape(input_tensor), dtype=dtype))\n",
        "    #print(perturbation.numpy())\n",
        "    \n",
        "    for i in range(num_approximation):\n",
        "        \n",
        "        softmax_accommodating_perturbation = model1(input_tensor+perturbation)\n",
        "        \n",
        "        #softmax_accommodating_perturbation_af = Activation(\"softmax\")(softmax_accommodating_perturbation)\n",
        "        # ノイズを足した配列とノーマルな配列がCNNで出力された結果のKL距離を求める\n",
        "        dist = tf.reduce_mean(kl_divergence(be_softmax,softmax_accommodating_perturbation))\n",
        "        #cross_entropy_accommodating_perturbation = -tf.reduce_sum(plain_softmax * tf.math.log(clipped(softmax_accommodating_perturbation))) * weight\n",
        "        adversarial_direction = tf.gradients(dist, [perturbation])[0]\n",
        "        pertubation = tf.stop_gradient(adversarial_direction)\n",
        "    #print(\"ok\")    \n",
        "    \n",
        "    pertubation = epsilon * get_unit_vector(pertubation)\n",
        "    \n",
        "    #corrent_softmax = model1(input_tensor) \n",
        "    vat_softmax = model1(input_tensor + perturbation)\n",
        "    loss =  tf.reduce_mean(kl_divergence(be_softmax,vat_softmax))\n",
        "    return  loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ-cuZIn4pjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = x_test[0:20]\n",
        "x = Activation('softmax')(model1(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiVdLxu6iXHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "0b8fabd9-cb8d-4d09-a2b3-70b898c6b358"
      },
      "source": [
        "file_path1 = pd.read_csv(\"/content/drive/My Drive/2048pix_tissue_choice/tran_6000.csv\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0b061f1c0c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_path1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/2048pix_tissue_choice/tran_6000.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/content/drive/My Drive/2048pix_tissue_choice/tran_6000.csv' does not exist: b'/content/drive/My Drive/2048pix_tissue_choice/tran_6000.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3TXST5i1Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB14Tuiv8M6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xi = []\n",
        "for i in range(10000):\n",
        "    print(np.argmax(x[i]))\n",
        "    xi.append(np.argmax(x[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7H0f-4j89gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xi.count(6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaXF0UBRNK1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for i in range(10):\n",
        "    x+=x[1][i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iuBxAVNo8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x[1][0]+x[1][1]+x[1][2]+x[1][3]+x[1][4]+x[1][5]+x[1][6]+x[1][7]+x[1][8]+x[1][9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TJB_IHbNtqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.argmax(x[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i_RcL6RNwim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}